---
title: "Opening an Isetan Department Store in Paris, France"
date: 2020-03-11
tags: [data science, foursquare]
excerpt: "IBM Data Science Certification Capstone Project"

---

## 1. Introduction
Perfect for Paris and arguably the trendiest department store in Japan, the flagship Isetan Shinjuku is renowned for having its window displays created by leading artists and offers only the finest in food, clothing and homeware. Working from the bottom up, you'll find the 'beauty apothecary' on the second basement floor, food and travel items on the other basement level, and the other seven floors dedicated to ladies' fashion and homeware.

This project seeks to fuse the best of Japanese and Parisian shopping. Paris's fashionistas demand the best of the Fat East and bringing Isetan to Paris delivers.

#### Data Requirements
20 Arrondissements Municipaux or districts comprise Paris, France. Utilizing Foursquare data, the areas the the highest potential for ideally placing an Isetan department store emerge.

## 2. Workflow
1. _Data Outline:_  
  A. District data for Paris including names, location data if available, and any other details required.  
2. _Obtaining Data:_  
  A. Research and find suitable sources for the district data for Paris. 
  B. Access and explore the data to determine if it can be manipulated for our purposes. 
3. _Data Wrangling and Cleaning:_  
  A. Clean the data and convert to a useable form as a dataframe. 
4. _Data Analysis and Location Data:_
  A. Foursquare location data will be leveraged to explore or compare districts around Paris. 
  B. Data manipulation and analysis to derive subsets of the initial data. 
  C. Identifying the high traffic areas using data visualisation and tatistical nalysis. 
5. _Visualization:_  
  A. Analysis and plotting visualizations. 
  B. Data visualization using various mapping libraries. 
6. _Discussion and Conclusions:_  
  A. Recomendations and results based on the data analysis. 
  B. Discussion of any limitations and how the results can be used, and any conclusions that can be drawn.

#### Library Import
```
import numpy as np # library to handle data in a vectorized manner
import json # library to handle JSON files
import pandas as pd
!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values
import requests # library to handle requests
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe
```

__Matplotlib and associated plotting modules__
```
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from bs4 import BeautifulSoup
```

__Import k-means from clustering stage__
```
from sklearn.cluster import KMeans
!conda install -c conda-forge folium=0.5.0 --yes 
import folium # map rendering library
print('Libraries imported.')
```

#### Paris, France Data Import
Paris' 20 Arrondissements Municipaux or Districts (Data Provided by Open|DATA)

```
paris = pd.read_csv('https://raw.githubusercontent.com/AR-data-science/Coursera_Capstone/master/Arrondissements_.csv')
paris
```

(Insert Output Photo)

#### Data Clean Up
__Column names__
```
paris.rename(columns={'NAME': 'Neighborhood ', 'CAR': 'Arrondissement_Num', 'Geometry_X': 'Latitude', 'Geometry_Y': 'Longitude',  'LAR': 'French_Name'}, inplace=True)
paris
```

(Insert Output)

__Removing Unnessary Columns__
```
paris.drop(['NSQAR','CAR.1','CARINSEE','NSQCO','SURFACE', 'PERIMETRE' ], axis=1, inplace=True)
paris
```

(Insert Output)

__Now that we have the dataframe ready, we continue to data analysis.__

# 3. Methodology and Exploratory Data Analysis
Check the dataframe's shape.
```
paris.shape
```

(20, 5)

#### Paris's Latitude and Longitude Determined by GeoPy Library

__Retrieve the Latitude and Longitude for Paris__
```
from geopy.geocoders import Nominatim 
address = 'Paris'
```
__Define the user_agent as Paris_explorer__
```
geolocator = Nominatim(user_agent="Paris_explorer")

location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude

print('The geographical coordinates of Paris France are {}, {}.'.format(latitude, longitude))
```

The geographical coordinates of Paris France are 48.8566969, 2.3514616.

#### Paris, France with Districts Overlayed

__Creating a map of Paris using the above latitude and longitude values__
```
map_paris = folium.Map(location=[latitude, longitude], zoom_start=12)
```

__Adding markers to the map__
```
for lat, lng, label in zip(paris['Latitude'], paris['Longitude'], paris['French_Name']):
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=25,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.3,
        parse_html=False).add_to(map_paris)  
    
map_paris
```

(Insert Output)

#### Using Foursquare's API
```
CLIENT_ID = 'Client ID Removed for Privacy' # your Foursquare ID
CLIENT_SECRET = 'Client_Secret Removed for Privacy' # your Foursquare Secret
VERSION = '20200311' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
```

Your credentails:
CLIENT_ID: Client ID Removed for Privacy
CLIENT_SECRET:Client_Secret Removed for Privacy

#### Exploratory Data Analysis
We explore the data and retrive the Top 100 Venues in 3eme Ardt, Temple Neighborhood and a long site list emerges to begin paring down the preferred potential locations.

#### Defining the Category Function

__Define a function that extracts the category of the venue__
```
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
```

#### Structuring the JSON File into a Pandas Dataframe

__Clean the JSON and structure it into a pandas dataframe.__
```
venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) # flatten JSON
```

__Filter columns__
```
filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]
```

__Filter the category for each row__
```
nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)
```

__Clean the columns__
```
nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]

nearby_venues.head(20)
```

(Insert Output)

__Check how many venues there are in 3eme Ardt within a radius of 500 meters__
```
print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))
```

100 venues returned by Foursquare

#### Creating a Nearby Venues Function
```
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
```

        __Create the API request URL__
```        
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
```

        __Make the GET request__
```        
        results = requests.get(url).json()["response"]['groups'][0]['items']
```
 
        __Return only relevant information for each nearby venue__
```     
       venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['French_Name', 
                  'Latitude', 
                  'Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
```

#### Creating a New Paris Venus Dataframe Called paris_venues
```
paris_venues = getNearbyVenues(names=paris['French_Name'],
                                   latitudes=paris['Latitude'],
                                   longitudes=paris['Longitude']
                                  )
```

(Insert Output)

#### Printing the paris_venues Dataframe
```
print(paris_venues.shape)
paris_venues.head(250)
```

(Insert Output)

#### Venues by Neighborhood
```
paris_venues.groupby('French_Name').count()
```

(Insert Output)

#### Number of Unique Venue Categories
```
print('There are {} unique venue categories.'.format(len(paris_venues['Venue Category'].unique())))
```

There are 197 unique venue categories.

### Neighborhood Analysis

#### Analyze each of the Neighborhoods from the results

__One hot encoding__
```
paris_onehot = pd.get_dummies(paris_venues[['Venue Category']], prefix="", prefix_sep="")
```

__Add neighborhood column back to dataframe__
```
paris_onehot['Neighborhood'] = paris_venues['French_Name'] 
```

__Move neighborhood column to the first column__
```
fixed_columns = [paris_onehot.columns[-1]] + list(paris_onehot.columns[:-1])
paris_onehot = paris_onehot[fixed_columns]

paris_onehot
```

(Insert Output)

__Dataframe Shape__
```
paris_onehot.shape
```

(1350, 198)

#### Rows by Neighborhood with Frequency of Mean Occurance by Category
```
paris_grouped = paris_onehot.groupby('Neighborhood').mean().reset_index()
paris_grouped
```

(Insert Output)

__Grouped Data's Shape__
```
paris_grouped.shape
```

(20, 198)

#### Printing each Neighborhood's Top 10 Most Common Venues
```
num_top_venues = 10

for hood in paris_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = paris_grouped[paris_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')
```

(Insert Output)

#### Placing the data into a Pandas Dataframe

__First sort the venues in descending order.__
```
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
```

#### Top 10 Venues by Neighborhood

__Create the new dataframe and display the top 10 venues for each neighborhood__
```
num_top_venues = 10
indicators = ['st', 'nd', 'rd']
```

__Created columns according to number of top venues__
```
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))
```

__Create the new Dataframe__
```
paris_venues_sorted = pd.DataFrame(columns=columns)
paris_venues_sorted['Neighborhood'] = paris_grouped['Neighborhood']

for ind in np.arange(paris_grouped.shape[0]):
    paris_venues_sorted.iloc[ind, 1:] = return_most_common_venues(paris_grouped.iloc[ind, :], num_top_venues)

paris_venues_sorted.head(20)
```

(Insert Output)

#### Sorted Data's Shape
```
paris_venues_sorted.shape
```

(20, 11)

#### Business Types by Client

__Categorical plot__
__Explore a plot of this data (a violin plot is used which is a density estimation of the underlying distribution).__
__The top 3 venue types as specified by the client for each neighborhood are used for the plotting.__

```
import seaborn as sns

fig = plt.figure(figsize=(50,25))
sns.set(font_scale=1.1)

ax = plt.subplot(3,1,1)
sns.violinplot(x="Neighborhood", y="French Restaurant", data=paris_onehot, cut=0);
plt.xlabel("")

ax = plt.subplot(3,1,2)
sns.violinplot(x="Neighborhood", y="Café", data=paris_onehot, cut=0);
plt.xlabel("")

plt.subplot(3,1,3)
sns.violinplot(x="Neighborhood", y="Wine Bar", data=paris_onehot, cut=0);

ax.text(-1.0, 3.1, 'Frequency distribution for the top 3 venue categories for each neighborhood (click to enlage)', fontsize=60)
plt.savefig ("Distribution_Frequency_Venues_3_categories.png", dpi=240)
plt.show()
```

(Insert Output)

#### Clothing Store Venue Analysis

__Add the Clothing_Store to explore this category__
```
import seaborn as sns

fig = plt.figure(figsize=(50,15))
sns.set(font_scale=1.1)

ax = plt.subplot(1,1,1)
sns.violinplot(x="Neighborhood", y="Clothing Store", data=paris_onehot, cut=0);
plt.xlabel("")

ax.text(1.0, 1.1, 'Frequency of Clothing stores for each neighborhood', fontsize=60)
plt.savefig ("Distribution_Frequency_Clothing_Venues.png", dpi=240)
plt.show()
```

(Insert Output)

#### Combining Three Categories

# Clothing_Store has been added to explore this category and make a comparison
```
import seaborn as sns

fig = plt.figure(figsize=(50,25))
sns.set(font_scale=1.1)

ax = plt.subplot(4,1,1)
sns.violinplot(x="Neighborhood", y="French Restaurant", data=paris_onehot, cut=0);
plt.xlabel("")

ax = plt.subplot(4,1,2)
sns.violinplot(x="Neighborhood", y="Café", data=paris_onehot, cut=0);
plt.xlabel("")

plt.subplot(4,1,3)
sns.violinplot(x="Neighborhood", y="Wine Bar", data=paris_onehot, cut=0);

plt.subplot(4,1,4)
sns.violinplot(x="Neighborhood", y="Clothing Store", data=paris_onehot, cut=0);

ax.text(-1.0, 3.1, 'Frequency distribution for the top 3 venue categories for each neighborhood (includes clothing)', fontsize=60)
plt.savefig ("Distribution_Frequency_Venues_3_categories_clothing.png", dpi=240)
plt.show()
```

(Insert Output)

### Highest-Potential Districts

__Download and put into a new dataframe called chosen_districts__
```
chosen_districts = pd.read_csv('https://raw.githubusercontent.com/AR-data-science/Coursera_Capstone/master/Week%205/Chosen.csv')
chosen_districts
```

(Insert Output)

### Map of Highest-Potential Districts

__Create a folium map of Paris with the 3 neighborhoods superimposed on the map__
```
map_chosen_districts = folium.Map(location=[latitude, longitude], zoom_start=13)
```

__Add Map Markers__
```
for lat, lng, label in zip(chosen_districts['Latitude'], chosen_districts['Longitude'], chosen_districts['French_Name']):
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=45,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.2,
        parse_html=False).add_to(map_chosen_districts)  
    
map_chosen_districts
```

(Insert Output)

#### Central Paris
```
map_chosen_districts = folium.Map(location=[latitude, longitude], zoom_start=15)
```

__Add markers to map__
```
for lat, lng, label in zip(chosen_districts['Latitude'], chosen_districts['Longitude'], chosen_districts['French_Name']):
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=130,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.2,
        parse_html=False).add_to(map_chosen_districts)  
    
map_chosen_districts
```

(Insert Map)

# Observations
Paris's city center is the clear winner and the spot deserving of expending potentially limited resources to focus in on an exact location.

# Inferences
With no exact science to picking a location, cross-referencing peripheral entertainment resources with clothing store density is likely one of the better means available to quickly narrow the search.

# Conclusion
While the opportunity to potentially further supplement the data with more cross-reference types or choosing a potentially more effective data reference mix exists, the results appear to align well with general expectations for high-potential areas, while adding a focus that a general idea of which area of Paris might be the best location could never achieve.

